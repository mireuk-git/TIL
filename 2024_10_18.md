![](tilThumb.wepb)
---
# 오늘의 코드카타
> SQL코드카타:
https://school.programmers.co.kr/learn/courses/30/lessons/59415
```
SELECT MAX(DATETIME) FROM ANIMAL_INS
```
또는
```
SELECT DATETIME
FROM ANIMAL_INS
ORDER BY DATETIME DESC
LIMIT 1
```

> 알고리즘 코드카타:
https://school.programmers.co.kr/learn/courses/30/lessons/12901
```
def solution(a, b):
    answer = ['FRI','SAT','SUN','MON','TUE','WED','THU']
    m=[31,29,31,30,31,30,31,31,30,31,30,31]
    d=0
    for i in range(a-1):
        d+=m[i]
    d+=b-1
    d%=7
    return answer[d]
```
2016년은 윤년이더라...

---
# 지도학습 - 분류모델
#### KNN
K-nearest neighbor(k개의 이웃)
데이터의 특성에 따라 그래프에 펼처놓는 과정을 하나의 학습이라 할 수 있음
테스트데이터가 들어왔을 때, 가장 가까운 K개의 데이터 중 가장 많은 클래스로 테스트데이터를 분류
-> 특성적으로 가장 비슷한 K개의 데이터 중 가장 많은 데이터가 속한 클래스로 분류

k값에 따라 클래스 분류가 얼마든지 변할 수 있다
k값에 따라 계산속도가 느려질 수 있음

> 필요 모듈
from sklearn.neighbors import KNeighborsClassifier

> 구현 과정
데이터전처리
데이터분할
스케일러 생성, 데이터 정규화
모델 = KNeighborsClassifier(n_neighbors=k)
- KNN모델 생성, k개 이웃
모델 학습
예측값 생성
평가

#### 나이브베이즈
베이즈 정리 기반 분류모델, 각 특징이 독립적이라고 가정
텍스트 분류 문제에서 사용
> 베이즈 정리
![](https://velog.velcdn.com/images/yw_j/post/a6591650-1a8d-4ee9-8fa2-753235a85c6d/image.png)
P(A|B): B일때 A일 확률
P(B|A): A일때 B일 확률
P(A): A가 일어날 확률
P(B): B가 일어날 확률

나이브베이즈 종류 - 취급하는 특징에 따라 다름
> 
- 가우시안 - 연속적이고 정규분포를 따르는 특징 (대부분 정규분포를 따르기에 대부분의 경우 이쪽 사용)
- 베르누이 - 이진수로 표현되는 특징
- 멀티노미얼 - 다항 분포를 따르는 특징

> 필요 모듈
from sklearn.naive_bayes import GaussianNB

> model = GaussianNB() - 가우시안 나이브베이즈 모듈 생성

#### 의사결정나무
데이터의 특징을 기준으로 의사결정 규칙 생성, 이를 데이터를 분류하거나 회귀하는데 사용
트리구조
- 노드: 데이터의 특정 특징에 대한 테스트
- branch: 테스트 결과
- leaf: 클래스 레이블

성능이 좋음, 대회에서 자주 사용됨

분할기준
> - 정보이득
엔트로피(불확실성) 값으로 데이터를 나누는 기준, 엔트로피가 가장 적은 기준 채택
![](https://velog.velcdn.com/images/yw_j/post/881c6ad5-80ae-4e64-9b08-7a59abf4c690/image.png)
- 지니계수
불순도 측정, 지니계수가 가장 적은 지점의 기준 채택
![](https://velog.velcdn.com/images/yw_j/post/82af271d-d978-46a1-a693-307a67807694/image.png)

더이상 낮아지지 않는 지점까지 학습 진행

> from sklearn.tree import DecisionTreeClassifier
모델=DecisionTreeClassifier(random_state=n)

# 비지도학습
데이터를 스스로 군집화하여 문제 해결
군집화
> - K-means clustering
- 계층적 군집화
- DBSCAN
차원축소
- PCA
- t-SNE
- LDA

## k-means clustering
학습단계랄게 없음...?
데이터를 어떻게 묶어야 잘 묶었다고 소문이 날까
가까운 데이터끼리(특징적으로 비슷한 데이터끼리) 묶는 기법
계산량이 좀 많을 수 있음(그래도 다른거에 비하면 그리 많은 편은 또 아님)

#### 알고리즘 단계
> 1. 초기화 - k개의 군집의 중심 랜덤하게 설정
2. 할당 - 각 데이터를 가장 가까운 군집에 할당
(군집이 먼저 만들어지고 거기에 포인트가 추가합류)
거리측정방법은 유클리드 거리 사용
3. 업데이트 - 군집의 중심을 데이터포인트의 평균으로 업데이트
4. 반복 - 더이상 군집중심이 변화하지 않을 때까지 반복

> K 설정 방법
1. 임의로 때려맞춘다(...)
2. 엘보우 방법
k를 증가시키면서 k에 대한 군집의 응집도 계산, 그 응집도가 급격히 감소하는 지점
![](https://velog.velcdn.com/images/yw_j/post/0ef2f1d1-d3a9-4a9d-b6b4-7e9bb61e8010/image.png)

#### 구현
> 데이터전처리
엘보우 기법 - 적절한 K값 찾기
얻은 K값 이용해 모델 학습
군집 시각화

> 엘보우 기법
```
inertia=[] #군집 응집도
K=range(1,11)
for k in K:
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(data_scaled)
    inertia.append(kmeans.inertia_)
plt.figure(figsize=(10,8))
plt.plot(K,inertia,'bx-')
plt.xlabel('k')
plt.ylabel('Inertia')
plt.title('Elbow Method For Optimal K')
plt.show()
```
범위 내의 모든 k에 대해
kmean 모델 생성, 생성된 모델에 학습 진행, 응집도 저장
구한 모든 응집도를 그래프로 그려 최적의 k 탐색

> 최적 K값으로 모델 학습
```
kmeans=KMeans(n_clusters=5, random_state=42)
kmeans.fit(data_scaled)
data['Cluster'] = kmeans.labels_
```
데이터프레임에 군집 결과 할당 - 어느 데이터가 어느 군집에 갔는지

> 군집시각화
```
	#연령 vs 소득
plt.figure(figsize=(10, 8))
sns.scatterplot(x=data['Age'], y=data['Annual Income (k$)'], hue=data['Cluster'], palette='viridis')
plt.title('Clusters of customers (Age vs Annual Income)')
plt.show()
	#소득 vs 지출점수
plt.figure(figsize=(10, 8))
sns.scatterplot(x=data['Annual Income (k$)'], y=data['Spending Score (1-100)'], hue=data['Cluster'], palette='viridis')
plt.title('Clusters of customers (Annual Income vs Spending Score)')
plt.show()
```
![](https://velog.velcdn.com/images/yw_j/post/151b0078-1eed-4ca0-8e71-6352420ace33/image.png)
소득 vs 나이
![](https://velog.velcdn.com/images/yw_j/post/7e42aa82-cf61-4ca2-bbd1-fcf2eb0916a6/image.png)
소득 vs 지출 점수, clustering이 제대로 안됐다. 

실습에서 확인되듯이 어떤 column을 이용하느냐에 따라 군집화가 얼마나 잘 적용되는지가 달라진다. 

> 모델 = KMeans(n_clusters = k, random_state=n)

matplotlib는 따로 안써두련다. 