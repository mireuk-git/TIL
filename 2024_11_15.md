---
# 오늘의 코드카타
>SQL 코드카타
https://leetcode.com/problems/replace-employee-id-with-the-unique-identifier/submissions/1453151986/
```
select U.unique_id, E.name
from Employees E left join EmployeeUNI U
on E.id = U.id
```

>알고리즘 코드카타
https://school.programmers.co.kr/learn/courses/30/lessons/12914
```
def solution(n):
    answer=[1,2,3]
    while len(answer)<n:
        answer.append(answer[-1]+answer[-2])
    return answer[n-1]%1234567
```
코드보다 문제해결을 위한 알고리즘, 공식을 생각해내는게 더 힘들었던 문제
a[n]은 n-2칸에서 2칸을 뛰어갈 때와 n-1칸에서 1칸을 뛰어갈 때의 경우로 나뉜다. 즉, 
a[n]=a[n-2]+a[n-1]


---
# LLM 특강
## LLM의 대두
기존 RNN을 LLM으로 쓸때의 문제점 - RNN 모델이 커지면서 기울기 소실 문제가 발생, 앞부분의 context가 뒷부분에 제대로 전달되지 않게 됨
RNN은 Long-Term Dependencies를 학습하기에 적합하지 않음
LSTM, GRU 등의 대안이 생겨났지만 구조적 한계로 완전 극복은 X


Transformer, Attention
RNN을 제외하고 Attention만을 모델로 만들었더니 성능이 더 뛰어나더라(Transformer모델)
병렬계산으로 학습속도 비약적으로 가속
장기의존성 학습에 적합


## LLM
오지게 큼. 200~300GB가 RAM에 올라가야함. 
굉장히 빠른 업데이트
Text Completion(문맥상 다음 단어 예측)에 특히 강함

장기의존성 문제에 매우 뛰어남(맥락이해 등)
아직 완벽하진 않음
프롬프트가 길어질수록 답변 부정확, 실행시간↑, 비용↑

프롬프트(prompt): LLM에 입력으로 주어지는 텍스트 데이터
Token: 프롬프트를 쪼갠 조각, 보통 단어단위
Tokenizer 참조
입출력 Token 개수에 따라 비용 결정

# 실습
관련 참고문서
https://platform.openai.com/docs/quickstart?language-preference=python

---

# VectorDB
Faiss - 벡터 검색 엔진, VectorDB 구현에 자주 사용됨

# RAG
LLM+검색시스템
(검색시스템과 비교했을때 맥락있는 답변 생성 가능하단 점이 차별됨)
학습 후의 최신 데이터 반영해 더 정확하게 답변 가능
개인정보 등 사용하기 힘든 데이터도 쉽게 사용

Retrieval(검색단계) - 질문을 임베딩, 가장 유사한 벡터를 검색결과로 사용
Generation(생성단계) - 검색된 문서를 LLM에 전달, 답변 생성

VectorDB+LLM=RAG?

필요한 모듈
>
from sentence_transformers import SentenceTransformer - 임베딩모듈
import numpy as np

인코딩
>
```
#multilingual-e5-large-instruct 모델 사용
model = SentenceTransformer('intfloat/multilingual-e5-large')
>
#문장 임베딩 진행
#sentences는 임베딩할 문장의 리스트
#embeddings는 임베딩된 문장의 벡터 리스트
embeddings = model.encode(sentences)
```
multilingual-e5-large - 다국어 문장 표현에 최적화된 e5모델