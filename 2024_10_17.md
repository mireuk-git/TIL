![](tilThumb.webp)

---
# 오늘의 코드카타
> SQL코드카타:
https://school.programmers.co.kr/learn/courses/30/lessons/131536#qna
```
SELECT USER_ID, PRODUCT_ID
FROM ONLINE_SALE
GROUP BY USER_ID, PRODUCT_ID
HAVING COUNT(*)>1
ORDER BY USER_ID ASC, PRODUCT_ID DESC
```
USER\_ID, PRODUCT\_ID가 모두 같은 행끼리만 그룹화하여 그 그룹의 행 개수가 1을 넘어가는 것들만 출력
...출력할때 행이 복수로 보이지 않는건 이미 그룹으로 묶였기 때문이다. COUNT(*) 출력해보던가.  

> 알고리즘 코드카타: 
https://school.programmers.co.kr/learn/courses/30/lessons/138477
```
def solution(k, score):
    answer = []
    fame=[]
    for i in score:
        if (len(fame)<k): 
            fame.append(i)
        else: 
            if fame[-1]<i: fame[-1]=i
        fame.sort(reverse=True)
        answer.append(fame[-1])    
    return answer
```
---
# 지도학습모델
> - 회귀모델
  - 선형회귀
  - 다항회귀
  - 리지회귀
  - 라쏘회귀
- 분류모델
  - 로지스틱 회귀
  - SVM
  - KNN
  - 나이브베이즈
  - 의사결정나무
  
## 회귀모델
#### 선형회귀
어제 구현을 하고 노트에 정리를 해보려다 포기해버렸기 때문에 여기부터 다시한다. 
> 사용 모듈
from sklearn.model_selection import train_test_split
 - 입력 데이터 분할
from sklearn.linear_model import LinearRegression
 - 선형회귀 모델
from sklearn.metrics import mean_squared_error, r2_score
 - 모델 평가용

> 구현과정
데이터 준비
훈련데이터, 테스트데이터, 레이블, 정답 = train_test_split(독립변수, 종속변수, test_size=float,테스트비율, random_state=n) - 입력데이터를 학습용/테스트용으로 분리
- random_state는 시드 같은 느낌인가보다
모델=LinearRegression() - 선형회귀모델 생성
모델.fit(학습용데이터, 레이블) - 학습 진행
모델.predict(테스트데이터) - 해당 독립변수에 대한 예측 반환
mean_squad_error(정답, 예측값) - 평가 방법, 0에 가까울수록 정확
r2_score(정답, 예측값) - 평가방법, 1에 가까울수록 정확

#### 다항회귀
선형회귀가 선형관계를 모델링했다면 다항회귀는 비선형관계를 모델링한다
차수 선택의 문제 - 차수가 높을수록 모델 복잡, 과적합 위험

> 사용 모듈
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import PolynomialFeatures
 - 다항회귀 모델
from sklearn.linear_model import LinearRegression
 - ~~? 이거 선형이잖아~~ 아무래도 n차함수로 만든 데이터를 전달하면 다항회귀가 되는듯
from sklearn.metrics import mean_squared_error, r2_score

> 구현과정
데이터 준비
poly=PolynomialFeatures(degree=n)
- n차함수 생성
- degree - 계수 설정
모델.fit_transform(x) 
- 데이터를 모델에 맞게 n차함수화
데이터 분리
모델 생성
모델훈련
테스트 데이터로 예상값 생성
예상값과 정답 비교하며 모델 평가

## 분류모델
### 로지스틱 회귀
결과가 둘중 하나일때 사용되는 통계기법
결과값이 0~1 사이에 위치해야 함(**확률**), 시그모이드 함수 사용
즉 어느쪽에 더 가깝냐, 라는 논리로 
> 시그모이드 함수
![](https://velog.velcdn.com/images/yw_j/post/2695692e-2742-4b7f-957e-7164a085bf15/image.png)

#### 비용함수
aka 로그손실함수, 크로스 엔트로피 손실 함수
모델의 예측 확률과 실제 레이블 사이의 **차이**를 측정하는 함수
1에 가까울수록 정확, 0에 가까울수록 부정확

> 사용 모듈
from sklearn.datasets import load_breast_cancer
from seaborn import load_dataset_titanic
 - 특정 데이터셋을 가져옴, 많이 쓸지는 몰루
from sklearn.preprocessing import StandardScaler
 - 데이터 정규화(0~1 사이로 맞춤)

> 구현 과정
데이터준비
sklearn에서 가져온 데이터셋은 .data와 .target으로 각각 데이터와 목표치를 가져올 수 있다. 
데이터 분할
scaler=StandardScaler() - scaler 객체 생성
scaler.fit_transform(훈련데이터) - 평균과 편차를 찾아 정규화
scaler.transform(test데이터) - test데이터는 통계데이터를 내지 않는다. 
LogisticRegression() - 로지스틱 회귀 모델 생성
모델학습
테스트데이터에 대한 예측값 생성
평가

### SVM
Soft Vector Machine
분류, 회귀분석에 사용
결정 경계(결정 초평면)을 찾아 분류
데이터를 구분하는 평면(선)을 그었을 때, 반대 클래스의 서로 가장 가까운 선의 거리가 최대가 되게

커널함수: 데이터를 더 높은 차원으로 매핑, **데이터를 분리**

> w*x-b=0
w - 가중치벡터
x - 입력벡터
b - 절편

> 필요 모듈
from sklearn.svm import SVC

> 구현 과정
데이터 전처리
데이터 정규화(scaler)
모델=SVC(kernel=커널함수)
 - kernel='linear' - 커널 함수를 linear로 사용
학습
예측
평가