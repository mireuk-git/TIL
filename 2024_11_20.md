![](tilThumb.webp)
# 오늘의 코드카타
> sql
https://leetcode.com/problems/employee-bonus/solutions/5302949/mysql-left-join-beats-98/
```
select E.name, B.bonus
from Employee E left join Bonus B on E.empId=B.empId
where B.bonus is null or B.bonus<1000
```

> 알고리즘
https://school.programmers.co.kr/learn/courses/30/lessons/87390
```
def solution(n, left, right):
    mat=[[0 for i in range(n)] for i in range(n)]
    for i in range(1,n+1):
        for j in range(i):
            mat[i-1][j]=i
            mat[j][i-1]=i
>    
    array=[]
    for i in mat:
        array+=i
>    
    answer = array[left:right+1]
    return answer
```
직접 2차원 배열을 만드는 코드, 시간초과로 실패
>
```
def solution(n, left, right):
    answer = []
    for j in range(left, right+1):
        x,y = j//n+1, j%n+1
        if x>=y: answer.append(x)
        elif y>x: answer.append(y)
    return answer
```
배열을 만들지 않고 left값과 right값 사이의 i 유추

--- 

# LLM 특강
## RAG
Retrieval-Augmented Generation
모델이 학습하지 못한 정보는 대답하지 못함

사용자가 질문하면 문서를 검색해 retrieve, 답변 작성
문서에서 연관된 정보 검색, LLM의 요청에 retrieve, 답변 생성

Multi-Modal을 이식한다면 이미지도 인식할 수 있다. 
기본적으론 텍스트 정도만 인식

embedding
데이터를 벡터뭉치로 변환하는 것, 또는 그 결과물
유사한 의미는 유사한 벡터


### 문서 저장 단계
Load > Split > Embed > Store
문서를 Split(청킹), 임베딩, 벡터 상태로 저장

### 질의응답 단계
질문을 받으면 
벡터데이터베이스에서 거리가 가까운 문서들을 조회

---

# Ch3 과제
일어나서 제대로 된 질문을 준비해서 테스트 해보려니까 모듈이 처참히 망가져있더라.
또 그놈의 faiss가 문제였는데 왜 import되는지는 아직도 모른다. 
faiss-cpu와 faiss-gpu 둘 다 다시 지우고 시도하는 중인데 어떻게든 설치에 성공한 어제와 달리 오늘은 설치도 안되고 있다. 

일단 오늘 특강에서 가져온 AnythingLLM을 RAG가 탑재된 LLM 모델로 치고 예상답안으로 적어서 냈다. 

일단 사용한 입력 프롬프트는 이거다. 
> OECD가 발표한 G7에 대해서 설명해줘. 

분명 좋은 입력 프롬프트는 아닐거다. 

AnythingLLM이 출력한 예상답안은 PDF를 참고해서 원하는대로 AI툴킷에 대해 답변을 생성해냈지만,  
일반 Gemini에게 물어봤을 때는 AI툴킷에 대한 내용이 아닌 G7 국가들의 정치에 관한 보고서와 관련된 내용을 출력해냈다. 

#### RAG가 필요한 이유
일반적으로 LLM은 자신이 학습하지 못한 지식은 답변으로 작성할 수 없으며 질문을 받았을 때 제대로 된 답변을 생성해내지 못하고 엉뚱한 답안을 출력할 가능성이 높다. 

이때 RAG가 있다면 벡터 데이터베이스에서 관련된 내용을 검색해 답변 작성에 활용할 수 있다. 

---

## 해설 특강
3. 
PyPDFLoader - pdf를 불러올 수 있는 라이브러리

4. 
chunking - 긴 문서를 작고 관리하기 쉬운 부분들로 나누기
검색 정확도 향상, LLM이 처리 가능한 컨텍스트 길이로 제한

>Tokenizer - LLM에 넣기 전에 단어 단위로 쪼개기(더 잘게 쪼갬)
Chunking - 검색이 쉽게 의미 단위로 쪼개기
둘이 다르다

고려사항: 
청크 크기가 너무 작으면 문맥이 끊김, 너무 크면 정보검색에 난항
중복 - 문맥 유지에 필요(너무 크면 메모리낭비, 작으면 무의미)
가급적이면 의미가 완결된 단위로 분할되어야 함

CharacterTextSplitter
구분자 기준으로 split

RecursiveCharacterTextSplitter
문단 > 문장 > 단어 순으로 split

5. embedding
chunk를 실수벡터형태로 표현
임베딩 간의 거리를 계산해 이들간의 의미적 관계 해석

6. 벡터 스토어 생성
임베딩 한 값을 벡터 데이터베이스에 저장

7. Retriever
~~가서 검색결과 물어와~~
저장된 벡터 데이터베이스에서 사용자의 질문과 관련된(유사벡터를 갖는) 문서를 가져오는 검색기

상위 문서 선정: top-k
가장 유사도가 높은 문서 k개 retrieve

>```
from langchain.vectorstores.base import VectorStore
>
#가장 유사한, 상위문서 10개
retriever = vectorstore.as_retriever(search_type="similarity", search_kwargs={"k": 10})
```
8. 프롬프트 템플릿 정의
>```
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
>
# 프롬프트 템플릿 정의
contextual_prompt = ChatPromptTemplate.from_messages([
    ("system", "Answer the question using only the following context."),
    ("user", "Context: {context}\\n\\nQuestion: {question}")
])
```
system - 모델이 어떻게 대답할지 system prompt 설정
context - 유동적으로 질문을 받음


---
# LangChain

LLM 모델을 사용해 다양한 언어에 대한 체인을 생성하는 프레임워크
서비스에 대한 다양한 컴포넌트들(모듈화!)을 연결해줌

모듈화된 컴포넌트 - 필요에 따라 조합 가능, 재사용성과 확장성이 높음
각 컴포넌트를 쉽게 연결 가능
여러 작업을 순차적으로 실행하는 체인, 상황에 따라 행동을 결정하는 에이전트 -> 복잡한 작업 자동화
다양한 언어모델, 벡터 DB와의 통합으로 데이터 소스 확장과 빠른 검색 가능

#### LLM
주어진 입력을 바탕으로 텍스트 생성

#### 프롬프트 템플릿
프롬프트를 동적으로 생성
입력값에 따라 템플릿이 채워저 모델에 전달, 반복작업 단순화
ex) system: 이 문서 읽고 여기 안의 내용 내에서만 대답해. 

#### 체인
여러 단계를 거치는 워크플로우를 하나로 묶는 기능
여러 컴포넌트를 하나로

#### 에이전트
동적으로 필요한 작업을 결정하고 수행
상황에 따라 행동 결정
답변하기 위해 API 호출이 필요한지, 텍스트 생성이 필요한지 판단

#### 벡터DB
지식(벡터덩어리)창고
