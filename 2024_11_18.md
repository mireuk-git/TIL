![](tilThumb.webp)

---
# 오늘의 코드카타
>SQL 코드카타
https://leetcode.com/problems/rising-temperature/
```
select w1.id
from Weather w1 join Weather w2
where datediff(w1.recordDate,w2.recordDate)=1 and w1.temperature>w2.temperature
```
오늘의 온도를 구하는 w1과 어제의 온도를 구하는 w2를 이용한다. 
w1의 날짜 데이터와 w2의 날짜 데이터의 차가 1을 유지하면서
w1의 온도가 w2의 온도보다 높으면 선택
>
사용하진 않았지만 기록해둔다
LAG(참조열) over (partition by 그룹화기준, order by 정렬기준)
이전행 참조
LEAD(참조열) over (partition by 그룹화기준, order by 정렬기준)
다음행 참조
각각 정렬기준을 기준으로 이전행/다음행의 참조열 데이터를 가져온다. 
partition by는 필수가 아니지만 order by는 필수

> 알고리즘
https://school.programmers.co.kr/learn/courses/30/lessons/131701
```
def solution(elements):
    answer = list(elements)
    for i in range(2,len(elements)):
        for j in range(len(elements)):
            s=0
            for k in range(i):
                s+=elements[(j+k)%len(elements)]
            answer.append(s)
    answer.append(sum(elements))
    answer=set(answer)       
    return len(answer)
```
아래 입출력 예시 설명문대로 알고리즘이 구현되었을 때의 코드
역시나 3중반복문의 위엄으로 시간초과 달성. 기대조차 안했다. 
```
def solution(elements):
    circular=elements*2
    answer=[]
    for i,n in enumerate(elements):
        answer.append(n)
        for j in circular[i+1:i+len(elements)]:
            n+=j
            answer.append(n)      
    return len(set(answer))
```
수열합 계산 순서를 바꿔 시작점을 기준으로 계산한 코드
2중반복문이 되었으며, 기타 반복문에 포함되는 계산을 최소화시켰다. 
호출마다 인덱스를 계산하는 대신 리스트의 길이를 두배로 하거나 먼저 계산하는 방법을 골랐다. 

---
# LLM 특강
## 프롬프트 엔지니어링
LLM이 원하는 대답을 출력하게 하기 위해 입력(prompt)을 어떻게 줘야 하는가

더 정확한 답변을 위해 프롬프트를 구체적으로 작성할 필요가 있음
- 복잡한 지시를 줄 때 단계 나누기
- 예시 주기(shot)
- "생각할 시간" 주기 - 단계 나누어 먼저 문제를 풀게 하고 결론을 내리게
- 외부도구 사용사용
- 신뢰구간 - 샘플 갯수가 적을수록 프롬프트의 성능 향상이 더 확실하게 보여야 한다

기타 자잘한 팁
>1. 청중 설정하기 ex. "0살짜리 어린이에게 설명하듯" 
2. "~하지마"가 아니라 "~해"라는 긍정문
3. 팁설정 "더 나은 답변을 주면 $x 팁을 줄게"
4. 임무설정 ex. "간단한 언어를 사용해"
5. 비교하기
6. 인간적인 방식 요청 ex. "자연스럽고 인간적인 방식으로 주어진 질문에 답해"
7. 편견제거 ex. 질문 후 "Ensure that your answer is unbiased and avoids relying on stereotypes"
8. 질문시키기 정보가 충분할때까지 질문을 하라고 시킴
9. 테스트추가 ex. 지식뒤에 테스트 추가 요청(방법을 알려주고 마지막에 테스트를 포함한 답변을 한 후 답변을 미리제공하지 않고, 답변이 정확한지 알려줘)
10. 구분기호 사용 및 키워드 제시하기('단어',"의미" 등)
11. 특정 단어 반복하기(주제어 반복하여 질문)
12. 필요한 모든 정보 추가하여~"명령어"
13. 텍스트 개선을 요청
14. 여러개의 파일이 있을 경우 여러 파일을 만드는 스크립트(python) 요청
15. 제시어 기반 글(특정 단어, 구, 문장을 사용해 텍스트 생성) ex. 제공된 단어를 바탕으로 이야기를 완성해줘. 흐름을 일관되게 유지해줘.


신뢰구간
|테스트케이스|정답률|
10 - 30
100 - 10
1000 - 3
10000 - 1



Langsmith - LangChain Hub
프롬프트 팁

## 구조화된 추출
LLM 등장 후 비정형 데이터의 활용가능성 높아짐
OpenAI에서는 Structured Output으로 정형 데이터 추출 지원

#### python type hint
> ```
number: int=42
>
def func(name: str, age: int, b: int)->int:
```

식으로 변수와 함수에 type표시 가능

#### pydantic
데이터를 검사하고 구조화
> ```
from pydantic import BaseModel
>
class Student(BaseModel):
	name: str
    age: int
    grade: float
>
#grade가 float이 아니므로 에러발생
s = Student("minsu",18, "A")
```

## 함수 호출
GPT4o는 2023년까지의 데이터만 학습
다만 구글에서 검색하는 함수를 호출해 답변 생성
공식문서에서 실제 동작 코드 확인

>```
#구글에 search_query로 검색, 검색결과 반환
def search_google(search_query):
	#...
>
	return search_result

---

```
import os
from getpass import getpass

os.environ["OPEN_API_KEY"] = getpass("OpenAI API key 입력:")
```
API키 환경변수로 입력

```
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage

#모델 초기화
model = ChatOpenAI(model="gpt-4o-mini")
```

>The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable


---
# 텍스트처리
모델이 자연어를 이해하고 분석하기 쉽게 텍스트를 구조화, 정제

노이즈 제거 - 문법적 에러는 찾기 쉬우나(?) 의미적으로 에러를 찾기는 매우 힘듬, 문학적 표현은 더 어려움
문장의 구조나 형태를 일관되게 유지(언어적 특색이 날아가기에 항상 베스트는 아님, 유사한 의미를 찾아낼 때 필요)
불필요한 단어 생략해 효율성 확보

늘 성능이 올라가기만 하진 않는다, 적절한 판단 하에 도입할 것

데이터 특성, 문제에 따라 순서와 세부적 방법 달라질 수 있음
일반적 순서는
1. 대소문자 변환
2. 불필요 문자 제거
3. 텍스트 토큰화
4. 기타 정규화

#### 토큰화
텍스트를 문장, 단어, 서브단어 등 작은 단위로 분할
보통 단어, 서브단어 수준으로 토큰화

- 단어단위 토큰화: 공백을 기준으로 토큰화, 대부분의 언어에서 사용가능하나 일부 언어는 간단함과 속도가 희생되고 희귀단어가 많으면 어휘크기가 커짐
- 서브워드 토큰화: 단어를 더 작은 단위로, 의미 단위로 분할, 새로운 단어나 희귀단어 처리에 유리, 어휘크기 줄고 새 단어 처리가 효율적, 다만 분리되었을때 의미가 유지되지 않거나
- Sentence-level tokenization(문장단위 토큰화): 문맥 이해가 필요할때 유용, 요약이나 문장 경계가 모호한 언어에선 복잡도 증가, 긴 문장에선 경계 탐지 어려움

#### 정규화
텍스트를 일관성있게 표준화된 방식으로 변경
모델이 불필요한 변동에 혼란을 겪지 않게 함
- 소문자 변환
- 불필요한 기호, 특수문자 제거
- 형태소 분석, 어간 추출, 표제어 추출 → 통일된 기본형태로 변환, 단어의 통일, 다만 의미나 문법적 변형의 정확도가 떨어짐

#### 불용어 제거
불용어(정보를 많이 가지고 있지 않은 단어) 제거, 모델이 중요한 단어에 집중, 불용어가 중요한 의미를 가질 수도 있기에 신뢰도가 늘 100은 아님

#### 문장 분리, 길이 조정
문장을 적절하게 나누거나 길이 조정
문장이 너무 길 경우 메모리 제한이나 성능 저하


# 임베딩 기법

#### Bag of Words (BoW)
단어의 빈도만을 기반으로 텍스트 벡터화
단어 순서나 문맥 고려X
텍스트 데이터가 작다면 효율적, 간단한 문서분류, 텍스트 분석에 유용
단어 집합의 크기만큼 벡터 형성, 어휘가 크면 고차원 벡터 형성

#### Term Frequency-Inverse Document Frequency (TF-IDF)
단어 빈도, 단어의 중요도 반영
문서 내에서 자주 등장하지만 전체 문서에서 드물게 등장한다면 해당 문서에서 중요한 단어로 취급, 가중치 높게 할당
모든 문서에서 공통으로 등장하는 흔한 단어 제외
문서 내에서 의미있는 단어 강조
문맥 반영X, 어휘 수가 많아지면 고차원 벡터

#### Word2Vec, GloVe
단어간의 의미적 유사성 반영(지금까진 그냥 빈도, 중요도만), 유사의미라면 유사벡터
단어를 고차원 벡터로 변환, 단어 간 관계 학습
- Word2Vec: 주위 단어들(문맥)에 기반해 단어의 의미 학습, 저차원 밀집 벡터로 표현
메모리효율↑ 계산속도↑
학습 종료 후 새로운 단어 추가 X, 문장 전체를 고려하진 못함
  - CBOW - 주변단어로 중심단어 예측
  - SkipGram - 중심단어로 주변단어 예측
- GloVe: 동시 등장 행렬, 특정 단어가 다른 단어들과 얼마나 자주 나타나는지 고려, 전체 문맥을 살피며 단어의 관계 반영, 작은 데이터셋에서 성능 낮음, 학습 후 새로운 단어 추가 불가

#### Transformer 기반 임베딩(BERT, GPT)
문맥을 양방향으로 파악(?)
문장 전체 맥락 고려
계산비용 
대규모 모델, 학습과 추론속도 느림, 높은 계산 자원
사전 학습 모델 - 패러미터 많음, 메모리와 저장공간을 많이 차지

