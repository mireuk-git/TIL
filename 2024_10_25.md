![](https://velog.velcdn.com/images/yw_j/post/e179f1f1-f754-4380-9486-842a75c3619f/image.webp)

---
# 오늘의 코드카타
> SQL코드카타
https://school.programmers.co.kr/learn/courses/30/lessons/131118
```
SELECT R.REST_ID, REST_NAME, FOOD_TYPE, FAVORITES, ADDRESS, ROUND(AVG(R.REVIEW_SCORE),2) AS SCORE
FROM REST_INFO I JOIN REST_REVIEW R ON R.REST_ID=I.REST_ID
WHERE ADDRESS LIKE '서울%' 
GROUP BY REST_ID
ORDER BY SCORE DESC,FAVORITES DESC
```
문자열에 특정 문자열이 포함되는지 검사하는 방법에는 LIKE와 INSTR이 있는데, 해당 문제에서 LIKE는 사용이 가능하고 INSTR이 사용이 불가능한 이유는 알 수 없다. LIKE의 경우 '서울'로 시작하는 값만 반환하는 점에서 그나마 유추해 볼 수 있을만한건 데이터베이스에 '경기도 서울 근처' 이딴 값이 들어있다정도인데...

> COLUMN LIKE '문자열': 패턴 검색, 와일드카드(%,_) 사용가능
INSTR(COLUMN, '문자열'): 해당 COLUMN에 문자열이 포함되는지 확인, 문자열의 시작 위치를 반환(존재하지 않는다면 0반환)

>알고리즘 코드카타
https://school.programmers.co.kr/learn/courses/30/lessons/77484
```
def solution(lottos, win_nums):
    answer = [7,7]
    for i in lottos:
        if i in win_nums:
            answer[0]-=1
            answer[1]-=1
        if i == 0:
            answer[0]-=1
    for i in range(len(answer)):
        if answer[i]==7: answer[i]-=1
    return answer
```

---
# CNN
Convolutional Neural Network, 합성곱신경망
사람의 시각정보를 모델링 ,이미지 및 다차원 데이터 처리에 특화
이미지에서 특징을 추출하기 위한 값으로 학습된 필터를 통해 이미지 파일에서 특징을 추출
특징맵 -  픽셀에 필터값을 곱해서 생성, 특징이 더 잘 반영된 이미지라고 생각하면 편함
ANN과 달리 필터로 학습하기 때문에 픽셀간의 관계를 파악하는데 더 유리함

## 기본구조

합성곱 층(Convolutional Layer), 풀링 층(pooling Layer), FC층으로 구성
> - 합성곱 층: 입력이미지에 필터를 적용해 특징맵 생성, 이미지의 국소적 패턴 학습
- 풀링 층: 특징맵의 크기를 줄이고 중요 특징 추출, Max Pooling, Average Pooling
- 완전연결층: 출력레이어, 최종 예측 수행

#### 필터
이미지에서 특징을 뽑아내는데 특화된 작은 크기의 행렬
이미지 내에서 움직이면서 필터와 이미지의 국소부분의 점곱을 통해 특징맵 형성
필터 여러개를 동시에 쓰기도 함
이미지의 dege, corner, texture 등 다양한 국소적 패턴 학습

#### 풀링 레이어
특징 맵의 크기를 줄이고 중요한 특징을 추출
- Max Pooling: 필터 크기 내에서 최대값 선택, 중요한 특징 강조
- Average Pooling: 필터 크기 내에서 평균값 계산, 정보 손실 최소화

stride: 필터를 얼만큼 움직일지 정함
padding: pooling으로 사이즈가 너무 많이 줄거나 layer를 많이 설정할때 테두리에 추가 데이터를 넣어 이미지의 크기를 키움

#### 플래튼 레이어
2차원의 특징맵을 1차원으로 바꿔줌
출력층으로 ANN을 쓰기 때문 

## CNN 아키텍쳐
웬만하면 내가 잘 쓰지 않는 역사시간이다. 
1. LeNet: 합성곱 층과 풀링층 반복, 완결연결층 사용
2. AlexNet: ReLU 활성화 함수와 드롭아웃 도입
3. VGG: 깊고 규칙적인 구조의 아키텍쳐, 3*3필터로 깊이 증가

## 구현
```
class SimpleCNN(nn.Module):
    def __init__(self):
        super(SimpleCNN,self).__init__()
        self.conv1=nn.Conv2d(3,32,3,padding=1)		#입력채널 1, 출력채널 32, 커널크기 3*3
        self.pool=nn.MaxPool2d(2,2)					#풀링크기 2*2
        self.conv2=nn.Conv2d(32,64,3,padding=1)		#입력채널 32, 출력채널 64, 커널크기3*3
        self.fc1=nn.Linear(64*8*8,512)				#완전연결층
        self.fc2=nn.Linear(512,10)					#출력층
        
    def forward(self,x):
        x=self.pool(torch.relu(self.conv1(x)))
        x=self.pool(torch.relu(self.conv2(x)))
        x=x.view(-1,64*8*8)							#flatten
        x=torch.relu(self.fc1(x))
        x=self.fc2(x)
        return x
    
model = SimpleCNN()
```

nn.Conv2d(in,out,kernelsize*kernelsize) - CNN모델 생성
입력채널: 이미지가 흑백인 경우 1, RGB인 경우 3
torch.Size([배치크기,채널,가로,세로])


---
# 기초통계 및 선형대수학
### 데이터 종류

#### 질적 데이터 vs 양적 데이터
질적 데이터 - 범주로 나누는 값 → 막대 그래프, 파이차트, 히트맵
명목 척도: 순서나 크기의 개념 없이 단순한 분류나 구분만 가능한 데이터 
→ 원-핫 인코딩(순서비교가 의미없음)
서열 척도: 순서나 대소관계 의미가 있지만 비례관계가 없음 
→ 라벨 인코딩(상대지표 가능)

양적 데이터 → 숫자 그대로 사용, 데이터의 분포, 관계, 변화추이 시각화
등간 척도: 대소 관계와 함꼐 값들 간의 차이에 의미가 있는 데이터, 0은 절대적이지 않고 사람이 임의로 정한 기준 → 절대적 의미가 없기에 비율연산 지양, 표준화 지향, 차이를 강조한 특징 생성, 순서나 차이만을 반영
비율 척도: 비율비교가 가능한 데이터, 0이 절대적

이산형 데이터 vs 연속형 데이터
이산형: 정해진 범위 내의 특정 값
연속형: 연속적인 값, 값 사이 무한히 많은 세부적인 값 존재

### 데이터 중심지표
데이터의 대표값
- 평균(Mean) - 이상치에 민감
- 중앙값(Median) - 이상치에 영향을 덜받음
- 최빈수(Mode) - 이상치에 영향을 덜받음, 이산형 데이터 또는 범주형 데이터에 사용

분포 형태에 따른 중심지표비교
- 대칭: 좌우대칭, 평균값=최빈수=중앙값
- 왼쪽꼬리: 데이터가 왼쪽으로 치우침, 평균값<중앙값<최빈수
- 오른쪽꼬리: 데이터가 오른쪽으로 치우침, 최빈수<중앙값<평균값

데이터 표준화
분포를 평균이 0, 표준편차가 1이 되게 변환

데이터 중심지표에 따른 모델 선택
평균 중심의 데이터 -> 선형모델(선형 회귀, SVM)
중앙값 중심의 데이터 -> 비선형모델(로버스트 회귀, 랜덤 포레스트)
최빈값 중심의 데이터 
- 주어진 데이터의 주변 이웃 중 가장 많이 나타나는 값 - KNN
- 이산형 결과를 예측 - 로지스틱 회귀
- 범주형 데이터 기반 분류 - 나이브 베이즈

데이터 산포도
평균이나 중심값으로부터 데이터가 얼마나 퍼져있는지

편차: 각 데이터값과 평균 사이의 차이
분산: 편차 제곱의 평균, 클수록 데이터가 퍼져있음
표준편차: 분산의 제곱근, 단위는 원래 데이터와 같음
사분위수: Q1-하위25%지점, Q3-상위75%지점, 데이터의 분포를 한눈에 볼수있음

데이터 상관관계
- 양의 상관관계 
- 음의 상관관계
- 무 상관관계

상관계수: 상관관계 정도를 수치화한 값
피어슨 상관계수 - 선형일때 사용가능
r=1: 완전한 양의 상관관계
r=-1: 완전한 음의 상관관계
r=0: 무 상관관계, 머신러닝에서 변수를 제거해도 상관없음
상관관계가 높은 변수는 유사한 정보를 담고 있을 가능성이 큼
종속 변수와 상관관계가 높은 독립변수를 골라 모델의 성능 향상

확률, 확률 분포
P(A∪B)=P(A)+P(B)-p(A∩B)
P(A∩B)=P(A)×P(B|A)
P(B|A)=P(A∩B)/p(A)