![](tilThumb.webp)

---
# 오늘의 코드카타
> SQL 코드카타
https://school.programmers.co.kr/learn/courses/30/lessons/77487
```
SELECT * FROM PLACES
WHERE HOST_ID IN 
(SELECT HOST_ID FROM PLACES GROUP BY HOST_ID HAVING COUNT(ID) > 1)
ORDER BY ID ASC
```
단순하게 GROUP, HAVING을 쓰려고 했지만 GROUP 때문에 출력이 한번밖에 되지 않았다.
먼저 헤비유저인 HOST_ID를 검색한 뒤 그와 같은 값을 가지는 모든 PLACES행 출력

> 알고리즘 코드카타
https://school.programmers.co.kr/learn/courses/30/lessons/42862
```
def solution(n, lost, reserve): 
    students = [1]*(n+2)
    for i in lost : 
        if i in reserve : reserve.remove(i);continue
        students[i] = 0
    for i in sorted(reserve) :
        if  students[i-1] == 0 : students[i-1] = 1
        elif students[i+1] == 0 : students[i+1] = 1
    return sum(students)-2
```
여벌이 있는 학생 중 도난당한 학생은 빌려줄 수 없기에 미리 제외
앞순부터 차례로 빌려줄 수 있는 학생 중 없는 학생에게 빌려줄 수 있는 경우 빌려줌

---
tensor - PyTorch에서 사용하는 기본 자료구조, 기울기를 구할 수 있음, 학습에 필요한 데이터의 자료구조는 모두 tensor여야 함
행과 열이 여러개

# RNN
시계열, 시퀀스(순서) 데이터를 다루는데 특화
데이터가 순환(데이터의 출력값을 다음 입력값으로 사용)
은닉상태 - 시퀀스의 정보 저장, 다음 시퀀스 단계로 전달되어 새로운 은닉 상태 생성, 순서 정보 반영의 핵심


시퀀스의 각 단계에서 동일한 가중치를 공유해 시퀀스 패턴 학습
학습은 순전파, 역전파(BPTT)를 통해 가중치 학습
BPTT에서 모든 시점의 가중치가 동시에 업데이트

오래된 순으로 데이터가 소실(장기의존성 문제)

```
class SimpleRNN(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(SimpleRNN,self).__init__()
        self.rnn = nn.RNN(input_size, hidden_size, batch_first = True)	#RNN Layer
        self.fc = nn.Linear(hidden_size, output_size)					#출력층
        
    def forward(self,x):
        h0=torch.zeros(1,x.size(0), hidden_size)						#초기은닉상태
        out,_=self.rnn(x,h0)
        out=self.fc(out)
        return out
    
input_size = 1
hidden_size = 32
output_size = 1
model = SimpleRNN(input_size, hidden_size, output_size)
```

> self.rnn = nn.RNN(input_size, hidden_size, batch_first = True)
-batch_first=bool: 첫번째 batch인가? 

 
## LSTM
RNN의 장기의존성 문제 타파를 위해 개발된 변종
셀 상태(cell state) - 각 시점에서 정보의 흐름 조정, 정보 장기적으로 유지
게이트(gate) - 정보를 선택적으로 저장, 삭제, 새 정보가 얼마나 출력에 영향을 주는지를 기준으로 셀 상태에 반영될지 결정하는 역할
- input gate - 셀 상태에 입력 정보가 얼마나 반영될지 조정하는 역할
- output gate - 새로운 셀 상태와 입력데이터를 바탕으로 새로운 출력 생성
- forget gate - 셀 상태에서 쓸모없는 정보 삭제

입력+은닉상태 -게이트-> 셀상태 업데이트
즉 BPTT가 아니라 게이트 사용
게이트를 이용해 선택적으로 정보를 저장해 중요한 정보를 더 잘 반영
긴 시퀀스에서 상대적으로 장기의존성 문제에서 자유로움
단 복잡하고 학습이 오래걸림, 데이터 의존성이 있음

```
class SimpleLSTM(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(SimpleLSTM, self).__init__()
        self.lstm=nn.LSTM(input_size, hidden_size, batch_first = True)		#LSTM 레이어 생성
        self.fc=nn.Linear(hidden_size,output_size)	#출력층
    
    def forward(self, x):
        h0=torch.zeros(1,x.size(0), hidden_size) #초기은닉상태
        c0=torch.zeros(1,x.size(0), hidden_size) #초기셀상태
        out,_=self.lstm(x,(h0,c0))
        out=self.fc(out)
        return out

model = SimpleLSTM(input_size, hidden_size, output_size)
```

## GRU
LSTM의 변형 (RNN→LSTM→GRU)
셀상태와 은닉상태를 합쳐 하나로 활용, 구조 단순화
마찬가지로 BPTT가 아닌 게이트 사용
\+ 업데이트 게이트 - 상태 업데이트, 이전 상태와 새로운 정보 결합, 이전상태가 얼마나 반영될지 결정 
\+ 리셋 게이트 - 새로운 정보와 이전 정보 결합, 이전 정보를 얼마나 무시할지 결정

따로 구현되지 않았다. LSTM에서 초기셀 내지 초기은닉을 없애면 되려나? 

---

# 선형대수학 강의

cos유사도: 벡터간의 유사도 판별, 1에 가까울수록 두 벡터는 동일한 방향을 가지며 -1에 가까울수록 반대방향, 0이라면 수직

벡터 연산 활용: 
Word Embedding - 단어를 벡터로 바꿔서 학습

cos유사도 활용: 
문서 유사도 측정
- TF-IDF: 텍스트 데이터에서 단어의 빈도를 벡터로 표현, 문서간의 유사성 측정
- 문서 임베딩: 딥러닝 모델에서 나온 특정 벡터를 통해 문서간의 유사성 측정
이미지 검색
- 이미지의 특징 벡터 비교, 유사한 이미지 검색 혹은 분류
추천 시스템
- 사용자와 아이템의 특징을 벡터로 표현해 유사한 사용자나 아이템을 찾아 비슷한 특징을 가진 것들끼리 매칭

행렬의 활용
이미지 처리
- 픽셀값으로 이루어진 2차원 행렬로 표현, 이미지를 해석하거나 변환할 수 있는 기초 데이터
뉴런의 활성화 값 결정
- 입력데이터와 가중치 처리의 가장 기본적인 방법