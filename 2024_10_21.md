![](tilThumb.webp)
---
# 오늘의 코드카타
> SQL코드카타:
https://school.programmers.co.kr/learn/courses/30/lessons/164672#qna
```
SELECT BOARD_ID, WRITER_ID, TITLE, PRICE, 
CASE WHEN STATUS='SALE' THEN '판매중'
WHEN STATUS='RESERVED' THEN '예약중'
ELSE '거래완료' END STATUS
FROM USED_GOODS_BOARD B
WHERE CREATED_DATE='2022-10-05'
ORDER BY BOARD_ID DESC
```
또는 
```
SELECT BOARD_ID, WRITER_ID, TITLE, PRICE, 
CASE STATUS
WHEN 'SALE' THEN '판매중'
WHEN 'RESERVED' THEN '예약중'
ELSE '거래완료' 
END STATUS
FROM USED_GOODS_BOARD B
WHERE CREATED_DATE='2022-10-05'
ORDER BY BOARD_ID DESC
```

> 알고리즘 코드카타:
https://school.programmers.co.kr/learn/courses/30/lessons/42840
```
def solution(answers):
    answer = []
    pattern=[[1,2,3,4,5],
             [2,1,2,3,2,4,2,5],
             [3,3,1,1,2,2,4,4,5,5]]
    score=[0,0,0]
    for i in range(len(answers)):
        for j in range(3):
            if answers[i]==pattern[j][i%(len(pattern[j]))]: score[j]+=1
    for i in range(3): 
        if max(score)==score[i]: answer.append(i+1)
    return answer
```
완전탐색(aka.BruteForce) - 가능한 모든 방법을 악으로깡으로 모두 확인하기

---
# 비지도학습
## 계층적 군집화
데이터포인트를 계층 구조로 그룹화

> 1. 데이터포인트 간의 거리 행렬 계산
2. 거리 행렬을 기반으로 가장 가까운 군집 병합, 가장 멀리 떨어진 군집 분할
3. 군집화 과정 덴드로그램으로 시각화

![](https://velog.velcdn.com/images/yw_j/post/f92998d3-7e98-4997-97a0-e66cdc361b36/image.png)
덴드로그램 예시

> -  병합 군집화(Agglomerative Clustering)
   - 가장 가까운 군집을 병합
   - 구현이 간단하며 데이터포인트 수에 비례해 계산비용 증가
   - 분할군집화보다 흔함
- 분할 군집화(Divisive Clustering)
  - 가장 멀리 떨어진 군집을 분할
  - 구현이 복잡하며 병합군집화보다 효율적
  - 너무 복잡해서 지원하는 라이브러리가 많이 없음
  
> 필요 모듈
from sklearn.cluster import AgglomerativeClustering
import scipy.cluster.hierarchy as sch
-Dendrogram 생성 모듈
from sklearn.metrics import silhouette_score
-평가용 모듈

> - dendro = sch.dendrogram(sch.linkage(data,method='ward'))
\- 주어진 data를 이용해 덴드로그램 생성
- 모델 = AgglomerativeClustering(n_clusters=k, metric='euclidean',linkage='ward')
\- 병합군집화 모델 생성
\- n_clusters=k - k개의 클러스터로 묶음
\- metric='euclidean' - 거리를 유클리드거리로 계산한다는 뜻 같지만 모종의 이유로 사용했을때 오류가 발생하며 작동하지 않더라
\- linkage='ward' - 클러스터 간의 거리 계산, 클러스터 내의 퍼지는 정도가 최소가 되도록 병합
- array = 모델.fit_predict(data) - 모델 학습, fit과 predict 동시 진행, 리턴은 배열, 무슨 배열인지는 모르겠음
- plt.scatter(x[y_hc==0,0],x[y_hc==0,1],s=100,c='red',label='cluster 1')
\- 평범하게 그림그리는 함수다만 x[y_hc]의 정체를 알 수 없어 일단 들고온다. 
- silhouette_score(data,y_hc) - -1~1의 값으로 군집화가 얼마나 잘 되었는지 평가, -1에 가까울수록 그룹화가 잘못되고 1에 가까울수록 잘됨

분할군집화는 나중에 따로 알아보는걸로

## DBSCAN
Density-Based Spatial Clustering of Applications with Noise
밀도 기반 군집화 알고리즘

데이터 밀도가 높은 영역을 군집으로 간주, 낮은 곳은 노이즈로 처리

> 
1. 임의의 데이터포인트 선택
2. 선택한 데이터포인트의 eps(입실론) 반경 내의 모든 데이터포인트 탐색
3. eps 반경내 데이터수가 최소 샘플수보다 많으면 선택 데이터포인트를 중심으로 군집으로 간주
4. 적으면 노이즈로 간주
5. 모든 데이터 포인트가 처리될 때까지 과정 반복

밀도 기반이기에 비구형 군집을 탐지 가능
노이즈를 효과적으로 처리할 수 있으며, 군집 수를 사전에 지정할 필요 없음
대신 거리와 그룹 내 요소 수를 사전에 지정해야 하며 밀도가 적은 그룹은 상대적으로 군집화가 잘 진행되지 않음

> 필요모듈
from sklearn.cluster import DBSCAN

> 모델=DBSCAN(eps=n,min_samples=k)
-eps=n: 최대 반경 n까지의 데이터포인트를 군집화
-min_samples=k: 최소k개의 데이터포인트가 모여야 군집화, 미만이라면 노이즈 처리

## 차원축소모델
column당 1차원 - 30column이면 30차원이라고 한다. 
데이터의 '차원'을 낮춰주는 모델
해석하기가 쉬워짐, 머신러닝의 효율이 상승
물론 축소 전과 후의 데이터의 표현력의 차이는 없어야 한다

## PCA
Principal Component Analysis

데이터의 중심화 - column의 평균을 구해 column의 각 값과의 차이를 구함 → 각 특성의 평균 0, 분산 1
공분산 행렬 - 특징간의 관계 계산, 데이터의 분포 확인
고유값, 고유벡터 계산 - 데이터가 얼마나 분산되는지, 그 방향
주성분 선택 - 고유값이 큰 순서대로(데이터의 분산을 더 많이 설명), 고유값은 '좌표'가 된다, 전체 분산의 95% 이상을 설명한다면 선택
데이터변환 - 선택된 주성분을 사용해 데이터를 저차원공간으로 변환

*공분산: 두 확률변수의 선형관계를 나타내며, 
하나의 값이 상승할 때 다른 값도 상승한다면 양수
하나의 값이 상승할 때 다른 값이 하강한다면 음수
그 어느것도 아니라면 0의 값을 갖는 성질을 띈다. 

> 데이터 로드, 전처리
PCA수행
주성분 확인
PCA 결과 시각화

> 필요 모듈
from sklearn.decomposition import PCA

> 모델=PCA(n_components=f)
-전체 분산의 f%를 설명하는 주성분 선택
sns.scatterplot(x=X_pca[:, 0], y=X_pca[:, 1], hue=y, palette='viridis', legend=None)
-PCA결과 시각화
-주성분 1(0)과 2(1)를 기준으로 출력

> 주성분 관련 pca 속성
.n_components_ - 선택된 주성분의 수
.explained_variance_ratio_ - 각 주성분이 설명하는 분산비율

통계학을 좀더 공부하는게 이해에 도움이 된다는 듯. 

## t-SNE
t-Distributed Stochastic Neighbor Embedding

> 1. 고차원 공간에서의 유사성(확률, 가우시안 분포) 계산
2. 저차원 공간에서의 유사성(확률, t-분포) 계산
3. KL발산 최소화 - 두 유사성의 차이를 KL발산(경사하강법)을 이용해 최소화
4. 반복적 최적화

비선형 구조 탐지 - 통계적으로 분포 계산
지역적 유사성 보존, 다만 전체적인 데이터 구조가 왜곡될 위험 
고차원 데이터 시각화 - 고차원에서 저차원으로 차원축소가 일어나기에 구조와 패턴을 쉽게 이해할 수 있음
다양한 데이터 유형 처리 가능, 다만 처리 시간이 긺
동작시간이 긺

> 필요 모듈
from sklearn.manifold import TSNE

> 모델=TSNE(n_components=2, random_state=42)
-주성분 개수 2개?

## LDA
Linear Discriminant Analysis, 선형 판별 분석

차원축소와 분류가 동시에 진행
클래스간 분산 최대화, 클래스 내 분산 최소화
분류 문제에서 성능이 향상, 클래스간의 데이터 처리가 차이가 날때 더 두드러짐
선형변환 사용 - 간단한 방법을 사용해 결과를 해석하는건 쉬우나 다소 세밀도가 떨어질 수 있음
잡음에 민감함, 학습 데이터가 많이 필요함

1. 클래스별 평균 계산 - 클래스간 분산 행렬 계산에 사용
2. 클래스 내 분산 행렬 계산
3. 클래스 간 분산 행렬 계산
4. 고유값 및 고유벡터 계산 - 클래스 내 분산행렬의 역행렬과 클래스간 분산행렬의 곱의 고유값과 벡터 계산 → 차원이 축소되면서 클래스가 최대한 분리되는 기준(행렬)을 찾는 과정
5. 고유값이 큰 순서대로 고유벡터 정렬, 선형 판별 축 선택
	-고유값이 큰 순서대로(클래스간 분산 더 많이 설명), 클래스의 수 -1만큼의 축 선택
6. 선택된 선형판별축을 사용해 데이터를 저차원공간으로 변환

> from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
모델=LinearDiscriminantAnalysis(n_components=k)
-클래스의 수 -1만큼의 선형 판별 축 선택해서 모델 생성

---
# 생성형 AI 특강
## rule-based알고리즘 vs AI알고리즘
알고리즘 직접 구현 vs 입력과 출력의 관계를 학습해 알고리즘 구현

## 생성형 AI
예) Midjourney,DALL-E,Gen-3,ChatGPT,Gemini,Udio 
새로운 콘텐츠를 생성할 수 있는 인공지능 기술
prompt: 생성형 AI에 주어지는 입력데이터, 출력할 내용을 지시하고 방향을 설정하는 역할, order

claude - 코드 물어보면 대답 잘 해준단다. 써보자. 
perflexity - 답변을 근거와 함께 제출(!!)

## LLM
LargeLanguageModel(대규모 언어 모델)
텍스트 생성, 번역, 질문, 답변, 요약 등 다양한 언어 작업을 하나의 모델에서 수행
엄청난 양의 텍스트 데이터로 학습

- OpenAI - GPT시리즈, LLM에서 선두를 달리고 있음
- Google - Gemini
- Meta - LLaMA
  - 오픈AI의 표준, 오픈소스(가중치까지 공개)

시간이 흐를수록
파라미터 수↑
입력 단어 수 = 입력 토큰 수↑
멀티모달 - 입력할 때 텍스트 뿐만 아니라 파일, 사진, 오디오 또한 가능

#### 원리
입력과 출력 사이 조건부 확률 분포 학습
모든 토큰별 확률 예측, 그중 가장 가능성이 높은 토큰을 선택해 답변
말이 안되는 단어의 나열은 애초에 확률이 낮다는 논리

GPT: Transformer Decoder 사용
Autoregressive 원리 - 이전 시점의 출력을 현재 시점의 입력으로 사용, 시퀀츠 예측(EOS 토큰이 출력될 때까지)

#### 한계점
- 학습데이터 벗어나는 입력(Out-of-Distribution) - 학습데이터 분포를 벗어나는 입력은 대처 불가
- 데이터 편향(Data Bias) - 학습 데이터에 편향이 존재하면 그 관점의 답변을 더 자주 생성
- 데이터 오류(Data Error) - 오류 있는 학습데이터는 모델이 잘못된 출력을 생성하게 함
LLM은 이런 한계를 압도적인 데이터량으로 찍어누른다
- 모호한 지시 처리 - 질문에 대한 배경과 세부 정보, 구체적인 답변 예시로 사용자가 원하는 답변을 할 수 있게 유도
- 맥락 유지의 어려움 - 기존의 질문 및 답변의 요약을 입력해 맥락 유지
(사실 기존 대화내용을 저장하진 않고 그걸 통째로 prompt에 입력하는 방식)
- Hallucination - 문맥에 맞는 듯 하지만 실제로는 근거없는 내용을 생성
→외부 지식 및 데이터베이스의 내용을 프롬프트에 넣어 해당 내용에 근거해 답변 생성(RAG, Retrieval-Augmented Generation)
- 정보의 한계: 새로운 정보를 얻지 못함 → 인터넷에서 새로운 정보 검색해 대처

## 프롬프트 엔지니어링 기초
작업 내용에 대한 이해를 바탕으로 LLM과 협력해 원하는 결과물을 창출
프롬프트 - 작업 내용 정의, 배경 및 맥락 파악, 제약 조건 확인, 출력형식설정

명확한 지침 제공 - 중요한 세부 사항이나 문맥 및 제약 조건 명시
페르소나 부여 - 시스템 메시지를 사용해 모델에 페르소나 부여(...??)
![](https://velog.velcdn.com/images/yw_j/post/e3768b51-6f31-4958-a43e-30e9d477d888/image.png)
프롬프트 명시적 구조화